# Introduction

Answer Server uses IDOL technology to provide specific and concise answers to a user's questions.

After completing this tutorial, you will have a working end-to-end question answering system based on your indexed documents. The system includes curated answer (FAQ) management, as well as integration with LLMs for a fully on-premise/off-cloud generative AI-powered interaction with your data.

## PART I - Configure and run the `data-admin` deployment

Explore and deploy an end-to-end question-answering IDOL system.

Start [here](./PART_I.md).

## PART II - Answer questions with RAG

Set up **RAG** (Retrieval Augmented Generation), which uses a large language model (LLM) to generate answers by querying your IDOL Content for relevant trusted documents.

Start [here](./PART_II.md).

## PART III - Combine with NiFi ingest

Extend this containerized deployment to add NiFi and index more data to interrogate with IDOL Answer Server.

Start [here](./PART_III.md).

## PART IV - Conversations

Set up IDOL Conversation Server to enable a chat-style interaction that helps users get to the right answer.

<!-- Start [here](./PART_IV.md). -->

> COMING SOON!

## PART V - Answer questions with Answer Bank

Use IDOL Data Admin to create and administer a trusted store of reference questions and answers.

<!-- Start [here](./PART_V.md). -->

> COMING SOON!

## Next steps

Explore some advanced IDOL configurations, in the [showcase section](../../README.md#showcase-lessons).
